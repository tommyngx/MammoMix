import argparse
import os

# Suppress TensorFlow and CUDA warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ["XLA_FLAGS"] = "--xla_gpu_cuda_data_dir=/usr/local/cuda"
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_ENABLE_DEPRECATION_WARNINGS'] = 'FALSE'
os.environ['NO_ALBUMENTATIONS_UPDATE'] = '1'

import pickle
import numpy as np
import yaml
from PIL import Image
from functools import partial
from pathlib import Path
import datetime
from tqdm.auto import tqdm
import json

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
from transformers import (
    AutoImageProcessor,
    AutoModelForObjectDetection,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
)

from dataset import BreastCancerDataset, collate_fn
from utils import load_config, get_image_processor, get_model_type
from evaluation import get_eval_compute_metrics_fn

def load_config(config_path):
    """Load configuration from YAML file."""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config

class SimpleDatasetClassifier(nn.Module):
    """
    Simple CNN to classify which dataset an image belongs to.
    Much simpler than the complex router approach.
    """
    def __init__(self, num_classes=3, device='cuda'):
        super().__init__()
        self.num_classes = num_classes
        self.device = device
        self.class_names = ['CSAW', 'DMID', 'DDSM']
        
        # Simple CNN for dataset classification
        self.features = nn.Sequential(
            # First block
            nn.Conv2d(3, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 640 -> 320
            
            # Second block
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 320 -> 160
            
            # Third block
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 160 -> 80
            
            # Fourth block
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(4),  # 80 -> 4x4
        )
        
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256 * 4 * 4, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
        
        # Initialize weights
        self._initialize_weights()
        
        # Suppress initialization message
    
    def _initialize_weights(self):
        """Initialize weights properly."""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        """Forward pass."""
        features = self.features(x)
        logits = self.classifier(features)
        return logits

class SimpleMoE(nn.Module):
    """
    ÄÆ¡n giáº£n: classifier chá»n expert â†’ láº¥y output expert â†’ Ä‘Æ°a ra káº¿t quáº£
    KhÃ´ng phá»©c táº¡p, freeze expert, evaluate nhÆ° test.py
    """
    def __init__(self, expert_models, dataset_classifier, device):
        super().__init__()
        self.experts = nn.ModuleList(expert_models)
        self.classifier = dataset_classifier
        self.device = device
        self.expert_names = ['CSAW', 'DMID', 'DDSM']
        
        # Freeze ALL experts - khÃ´ng train gÃ¬ cáº£
        for expert in self.experts:
            for param in expert.parameters():
                param.requires_grad = False
            expert.eval()  # LuÃ´n eval mode
        
        # Freeze classifier luÃ´n - chá»‰ dÃ¹ng Ä‘á»ƒ route
        for param in self.classifier.parameters():
            param.requires_grad = False
        self.classifier.eval()  # LuÃ´n eval mode
        
        # Statistics
        self.routing_counts = torch.zeros(3, device=device)
        self.total_routed = 0
        
        # Suppress initialization message

    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):
        pass
    
    def gradient_checkpointing_disable(self):
        pass
    
    def forward(self, pixel_values, labels=None, return_routing=False):
        """
        ÄÆ N GIáº¢N nhÆ°ng PHáº¢I cÃ³ loss cho evaluation:
        1. Classifier chá»n expert cho má»—i áº£nh
        2. Gá»i expert tÆ°Æ¡ng á»©ng vá»›i labels Ä‘á»ƒ cÃ³ loss
        3. Gá»™p output láº¡i theo thá»© tá»±
        4. Tráº£ vá» vá»›i loss - XONG!
        """
        batch_size = pixel_values.shape[0]
        
        # 1. Classifier chá»n expert - LUÃ”N no_grad
        with torch.no_grad():
            dataset_logits = self.classifier(pixel_values)
            expert_choices = torch.argmax(dataset_logits, dim=1)
        
        # Statistics
        if not self.training:
            for choice in expert_choices:
                self.routing_counts[choice] += 1
            self.total_routed += batch_size
            
            if self.total_routed % 100 == 0:
                usage_pct = (self.routing_counts / self.total_routed * 100).cpu().numpy()
                print(f"Routing: CSAW={usage_pct[0]:.1f}%, DMID={usage_pct[1]:.1f}%, DDSM={usage_pct[2]:.1f}%")
        
        # 2. Group by expert - Ä‘Æ¡n giáº£n
        expert_groups = {}
        for i in range(batch_size):
            expert_idx = expert_choices[i].item()
            if expert_idx not in expert_groups:
                expert_groups[expert_idx] = []
            expert_groups[expert_idx].append(i)
        
        # 3. Gá»i expert cho tá»«ng group - Cáº¦N LOSS cho evaluation
        sample_to_output = {}  # Map sample index to output
        total_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        
        for expert_idx, sample_indices in expert_groups.items():
            expert_pixel_values = pixel_values[sample_indices]
            expert_labels = None
            if labels is not None:
                expert_labels = [labels[i] for i in sample_indices]
            
            # CRITICAL: Gá»i expert vá»›i labels Ä‘á»ƒ cÃ³ loss
            with torch.no_grad():
                expert_output = self.experts[expert_idx](expert_pixel_values, labels=expert_labels)
            
            # Accumulate loss náº¿u cÃ³
            if hasattr(expert_output, 'loss') and expert_output.loss is not None:
                weight = len(sample_indices) / batch_size
                total_loss = total_loss + expert_output.loss * weight
            
            # LÆ°u output cho tá»«ng sample
            for i, sample_idx in enumerate(sample_indices):
                sample_to_output[sample_idx] = {
                    'logits': expert_output.logits[i],  # [num_queries, num_classes]
                    'pred_boxes': expert_output.pred_boxes[i],  # [num_queries, 4]
                }
        
        # 4. Gá»™p láº¡i theo Ä‘Ãºng thá»© tá»± batch
        batch_logits = []
        batch_pred_boxes = []
        
        for i in range(batch_size):
            if i in sample_to_output:
                batch_logits.append(sample_to_output[i]['logits'])
                batch_pred_boxes.append(sample_to_output[i]['pred_boxes'])
            else:
                # KhÃ´ng cÃ³ output - táº¡o dummy (khÃ´ng nÃªn xáº£y ra)
                print(f"WARNING: No output for sample {i}")
                dummy_logits = torch.zeros((100, 2), device=pixel_values.device)  # YOLOS default
                dummy_boxes = torch.zeros((100, 4), device=pixel_values.device)
                batch_logits.append(dummy_logits)
                batch_pred_boxes.append(dummy_boxes)
        
        # Stack thÃ nh batch tensors
        batch_logits = torch.stack(batch_logits, dim=0)  # [batch_size, num_queries, num_classes]
        batch_pred_boxes = torch.stack(batch_pred_boxes, dim=0)  # [batch_size, num_queries, 4]
        
        # Ensure 4 dimensions for boxes
        batch_pred_boxes = batch_pred_boxes[..., :4].contiguous()
        
        # 5. Táº¡o output vá»›i loss - QUAN TRá»ŒNG cho evaluation
        from transformers.models.yolos.modeling_yolos import YolosObjectDetectionOutput
        
        # PHáº¢I cÃ³ loss cho evaluation
        final_loss = None
        if labels is not None:
            final_loss = total_loss
        
        combined_output = YolosObjectDetectionOutput(
            loss=final_loss,  # CRITICAL: Pháº£i cÃ³ loss
            logits=batch_logits,
            pred_boxes=batch_pred_boxes,
            last_hidden_state=None
        )
        
        if return_routing:
            routing_info = {
                'choices': expert_choices,
                'groups': expert_groups,
                'stats': self.get_routing_stats()
            }
            return combined_output, routing_info
        else:
            return combined_output
    
    def get_routing_stats(self):
        """Get routing statistics."""
        if self.total_routed == 0:
            return {name: 0.0 for name in self.expert_names}
        
        stats = {}
        usage = (self.routing_counts / self.total_routed).cpu().numpy()
        for i, name in enumerate(self.expert_names):
            stats[name] = float(usage[i])
        return stats
    
    def reset_routing_stats(self):
        """Reset routing statistics."""
        self.routing_counts.zero_()
        self.total_routed = 0

def load_expert_models(weight_dir, device):
    """Load expert models (suppress messages)."""
    expert_names = ['yolos_CSAW', 'yolos_DMID', 'yolos_DDSM']
    expert_paths = [os.path.join(weight_dir, name) for name in expert_names]
    
    models = []
    processors = []
    
    for path in expert_paths:
        # Suppress loading messages
        processor = AutoImageProcessor.from_pretrained(path)
        model = AutoModelForObjectDetection.from_pretrained(
            path,
            id2label={0: 'cancer'},
            label2id={'cancer': 0},
            auxiliary_loss=False,
        )
        model = model.to(device)
        model.eval()
        models.append(model)
        processors.append(processor)
    
    return models, processors

def create_dataset_with_labels(config, image_processor, dataset_name, split, epoch=None):
    """Create dataset with dataset labels (0=CSAW, 1=DMID, 2=DDSM)."""
    SPLITS_DIR = Path(config.get('dataset', {}).get('splits_dir', '/content/dataset'))
    MODEL_NAME = config.get('model', {}).get('model_name', 'hustvl/yolos-base')
    
    dataset = BreastCancerDataset(
        split=split,
        splits_dir=SPLITS_DIR,
        dataset_name=dataset_name,
        image_processor=image_processor,
        model_type=get_model_type(MODEL_NAME),
        dataset_epoch=epoch
    )
    
    # Create simple dataset labels
    dataset_map = {'CSAW': 0, 'DMID': 1, 'DDSM': 2}
    dataset_label = dataset_map[dataset_name]
    
    # Create labels for all samples
    labels = [dataset_label] * len(dataset)
    
    return dataset, labels

def train_dataset_classifier(config, device, epoch=None, weight_dir=None):
    """
    Train simple CNN to classify datasets.
    Much simpler than the complex expert evaluation approach.
    """
    print("\n" + "="*50)
    print("PHASE 1: Training Simple Dataset Classifier")
    print("="*50)
    
    MODEL_NAME = config.get('model', {}).get('model_name', 'hustvl/yolos-base')
    image_processor = AutoImageProcessor.from_pretrained(MODEL_NAME)
    
    # Create datasets with simple labels
    all_train_datasets = []
    all_train_labels = []
    all_val_datasets = []
    all_val_labels = []
    
    for dataset_name in ['CSAW', 'DMID', 'DDSM']:
        print(f"Loading {dataset_name} data...")
        
        # Train data
        train_dataset, train_labels = create_dataset_with_labels(
            config, image_processor, dataset_name, 'train', epoch
        )
        all_train_datasets.append(train_dataset)
        all_train_labels.extend(train_labels)
        print(f"{dataset_name} train: {len(train_dataset)} samples")
        
        # Val data
        val_dataset, val_labels = create_dataset_with_labels(
            config, image_processor, dataset_name, 'val', epoch
        )
        all_val_datasets.append(val_dataset)
        all_val_labels.extend(val_labels)
        print(f"{dataset_name} val: {len(val_dataset)} samples")
    
    # Combine datasets
    from torch.utils.data import ConcatDataset
    combined_train_dataset = ConcatDataset(all_train_datasets)
    combined_val_dataset = ConcatDataset(all_val_datasets)
    
    print(f"Combined train: {len(combined_train_dataset)} samples")
    print(f"Combined val: {len(combined_val_dataset)} samples")
    
    # Count distribution
    train_counts = np.bincount(all_train_labels, minlength=3)
    print(f"Train distribution: CSAW={train_counts[0]}, DMID={train_counts[1]}, DDSM={train_counts[2]}")
    
    # Create classifier
    classifier = SimpleDatasetClassifier(num_classes=3, device=device).to(device)
    
    # Training setup
    optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()
    num_epochs = epoch if epoch else 10
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    
    best_val_acc = 0.0
    best_epoch = 0
    
    # Custom dataset class for labels
    class LabeledDataset(Dataset):
        def __init__(self, base_dataset, labels):
            self.base_dataset = base_dataset
            self.labels = labels
        
        def __len__(self):
            return len(self.base_dataset)
        
        def __getitem__(self, idx):
            item = self.base_dataset[idx]
            item['dataset_label'] = self.labels[idx]
            return item
    
    # Custom collate function to handle dataset_label
    def custom_collate_fn(batch):
        # Extract dataset labels
        dataset_labels = [item['dataset_label'] for item in batch]
        
        # Remove dataset_label from items before using standard collate
        for item in batch:
            del item['dataset_label']
        
        # Use standard collate function
        collated = collate_fn(batch)
        
        # Add dataset labels back
        collated['dataset_label'] = dataset_labels
        
        return collated
    
    # Create labeled datasets
    train_labeled = LabeledDataset(combined_train_dataset, all_train_labels)
    val_labeled = LabeledDataset(combined_val_dataset, all_val_labels)
    
    # Training loop
    for epoch_idx in range(num_epochs):
        print(f"\nEpoch {epoch_idx + 1}/{num_epochs}")
        
        # Training
        classifier.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_loader = DataLoader(train_labeled, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)
        
        for batch in tqdm(train_loader, desc="Training"):
            pixel_values = batch['pixel_values'].to(device)
            dataset_labels = torch.tensor(batch['dataset_label'], device=device)
            
            optimizer.zero_grad()
            outputs = classifier(pixel_values)
            loss = criterion(outputs, dataset_labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            predictions = torch.argmax(outputs, dim=1)
            train_correct += (predictions == dataset_labels).sum().item()
            train_total += dataset_labels.size(0)
        
        scheduler.step()
        
        # Validation
        classifier.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        val_loader = DataLoader(val_labeled, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                pixel_values = batch['pixel_values'].to(device)
                dataset_labels = torch.tensor(batch['dataset_label'], device=device)
                
                outputs = classifier(pixel_values)
                loss = criterion(outputs, dataset_labels)
                
                val_loss += loss.item()
                predictions = torch.argmax(outputs, dim=1)
                val_correct += (predictions == dataset_labels).sum().item()
                val_total += dataset_labels.size(0)
        
        # Calculate accuracies
        train_acc = train_correct / train_total
        val_acc = val_correct / val_total
        
        print(f"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.4f}")
        
        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_epoch = epoch_idx + 1
            
            # Save classifier in moe_MOMO directory
            moe_save_dir = os.path.join(weight_dir, 'moe_MOMO')
            os.makedirs(moe_save_dir, exist_ok=True)
            classifier_save_path = os.path.join(moe_save_dir, 'classifier_best.pth')
            torch.save(classifier.state_dict(), classifier_save_path)
            print(f"Best classifier saved to {classifier_save_path} (acc: {val_acc:.4f})")
        
        # Test MoE after each epoch
        print(f"\n=== Testing MoE with Current Classifier (Epoch {epoch_idx + 1}) ===")
        test_moe_with_classifier(config, classifier, device, epoch_idx + 1, weight_dir)
    
    print(f"\nBest classifier accuracy: {best_val_acc:.4f} (Epoch {best_epoch})")
    
    # Load best classifier from moe_MOMO path
    moe_save_dir = os.path.join(weight_dir, 'moe_MOMO')
    best_path = os.path.join(moe_save_dir, 'classifier_best.pth')
    classifier.load_state_dict(torch.load(best_path, map_location=device))
    classifier.eval()
    
    return classifier

def test_moe_with_classifier(config, classifier, device, epoch_num, weight_dir, dataset_name=None):
    """Test MoE Ä‘Æ¡n giáº£n - chá»‰ inference nhÆ° test.py"""
    try:
        # Load expert models
        expert_models, _ = load_expert_models(weight_dir, device)
        
        # Create MoE - táº¥t cáº£ frozen
        moe_model = SimpleMoE(expert_models, classifier, device).to(device)
        moe_model.eval()
        
        # Dataset
        MODEL_NAME = config.get('model', {}).get('model_name', 'hustvl/yolos-base')
        image_processor = AutoImageProcessor.from_pretrained(MODEL_NAME)
        
        if dataset_name:
            test_dataset, _ = create_dataset_with_labels(config, image_processor, dataset_name, 'test')
            print(f"Testing MoE on {dataset_name}: {len(test_dataset)} samples")
        else:
            # Combined datasets
            test_datasets = []
            for ds_name in ['CSAW', 'DMID', 'DDSM']:
                test_dataset, _ = create_dataset_with_labels(config, image_processor, ds_name, 'test')
                test_datasets.append(test_dataset)
            
            from torch.utils.data import ConcatDataset
            test_dataset = ConcatDataset(test_datasets)
            print(f"Testing MoE on combined: {len(test_dataset)} samples")
        
        # Evaluation nhÆ° test.py - ÄÆ N GIáº¢N
        training_args = TrainingArguments(
            output_dir='./temp_eval',
            per_device_eval_batch_size=8,
            dataloader_num_workers=0,
            remove_unused_columns=False,
            report_to=[],
            fp16=False,
            bf16=False,
            eval_do_concat_batches=False,
            disable_tqdm=False,
        )
        
        eval_compute_metrics_fn = get_eval_compute_metrics_fn(image_processor)
        
        trainer = Trainer(
            model=moe_model,
            args=training_args,
            processing_class=image_processor,
            data_collator=collate_fn,
            compute_metrics=eval_compute_metrics_fn,
        )
        
        print("Running evaluation...")
        results = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix='test')
        
        # Print results
        test_name = dataset_name if dataset_name else "Combined"
        print(f"Epoch {epoch_num} MoE Results on {test_name}:")
        
        for key, value in results.items():
            if isinstance(value, (int, float, np.integer, np.floating)):
                print(f"  {key}: {float(value):.4f}")
            else:
                print(f"  {key}: {value}")
        
        # Routing stats
        stats = moe_model.get_routing_stats()
        routing_str = ", ".join([f"{name}={usage*100:.1f}%" for name, usage in stats.items()])
        print(f"Routing: {routing_str}")
        
        return results
        
    except Exception as e:
        print(f"MoE test failed: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_moe_like_train_moe2(config, classifier, device, dataset_name, expert_weights_dir, epoch=None):
    """Run MoE - KHÃ”NG TRAIN GÃŒ Cáº¢, chá»‰ inference nhÆ° test.py"""
    print(f"\n=== Testing SimpleMoE trÃªn {dataset_name} - KHÃ”NG TRAIN ===")
    
    try:
        expert_models, expert_processors = load_expert_models(expert_weights_dir, device)
        image_processor = expert_processors[0]
        
        # Create MoE - táº¥t cáº£ frozen
        model = SimpleMoE(expert_models, classifier, device).to(device)
        model.eval()
        
        # Save initial MoE
        moe_save_dir = os.path.join(expert_weights_dir, 'moe_MOMO')
        os.makedirs(moe_save_dir, exist_ok=True)
        
        initial_moe_path = os.path.join(moe_save_dir, 'moe_initial.pth')
        torch.save(model.state_dict(), initial_moe_path)
        print(f"MoE model saved to: {initial_moe_path}")
        
        # Create test dataset
        SPLITS_DIR = Path(config.get('dataset', {}).get('splits_dir', '/content/dataset'))
        MODEL_NAME = config.get('model', {}).get('model_name', 'hustvl/yolos-base')
        
        test_dataset = BreastCancerDataset(
            split='test',
            splits_dir=SPLITS_DIR,
            dataset_name=dataset_name,
            image_processor=image_processor,
            model_type=get_model_type(MODEL_NAME),
            dataset_epoch=epoch
        )
        
        print(f'Test dataset: {len(test_dataset)} samples')
        
        # Setup evaluation nhÆ° test.py
        import datetime
        date_str = datetime.datetime.now().strftime("%d%m%y")
        run_name = f"SimpleMoE_{dataset_name}_{date_str}"
        
        training_args = TrainingArguments(
            output_dir=moe_save_dir,
            run_name=run_name,
            per_device_eval_batch_size=8,
            dataloader_num_workers=0,
            remove_unused_columns=False,
            report_to=[],
            fp16=torch.cuda.is_available(),
            eval_do_concat_batches=False,
            disable_tqdm=False,
        )
        
        eval_compute_metrics_fn = get_eval_compute_metrics_fn(image_processor)
        
        trainer = Trainer(
            model=model,
            args=training_args,
            processing_class=image_processor,
            data_collator=collate_fn,
            compute_metrics=eval_compute_metrics_fn,
        )
        
        print(f"\n=== Testing SimpleMoE on {dataset_name} ===")
        test_results = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix='test')
        
        # Print results nhÆ° test.py
        print(f"\n=== Test Results on {dataset_name} ===")
        for key, value in test_results.items():
            if isinstance(value, float):
                print(f"{key}: {value:.4f}")
            else:
                print(f"{key}: {value}")
        
        # Routing statistics
        print(f"\n=== Routing Statistics ===")
        final_stats = model.get_routing_stats()
        
        # CRITICAL: Ensure classifier starts in correct mode
        model.classifier.eval()  # Keep classifier frozen during training
        
        # Save initial MoE model after creation
        moe_save_dir = os.path.join(expert_weights_dir, 'moe_MOMO')
        os.makedirs(moe_save_dir, exist_ok=True)
        
        initial_moe_path = os.path.join(moe_save_dir, 'moe_initial.pth')
        torch.save(model.state_dict(), initial_moe_path)
        print(f"Initial MoE model saved to: {initial_moe_path}")
        
        # Create datasets exactly like train_moe2.py
        SPLITS_DIR = Path(config.get('dataset', {}).get('splits_dir', '/content/dataset'))
        MODEL_NAME = config.get('model', {}).get('model_name', 'hustvl/yolos-base')
        
        print(f"Creating datasets for {dataset_name}...")
        train_dataset = BreastCancerDataset(
            split='train',
            splits_dir=SPLITS_DIR,
            dataset_name=dataset_name,
            image_processor=image_processor,
            model_type=get_model_type(MODEL_NAME),
            dataset_epoch=epoch
        )
        
        val_dataset = BreastCancerDataset(
            split='val',
            splits_dir=SPLITS_DIR,
            dataset_name=dataset_name,
            image_processor=image_processor,
            model_type=get_model_type(MODEL_NAME),
            dataset_epoch=epoch
        )
        
        test_dataset = BreastCancerDataset(
            split='test',
            splits_dir=SPLITS_DIR,
            dataset_name=dataset_name,
            image_processor=image_processor,
            model_type=get_model_type(MODEL_NAME),
            dataset_epoch=epoch
        )
        
        print(f'Train: {len(train_dataset)} samples')
        print(f'Val: {len(val_dataset)} samples') 
        print(f'Test: {len(test_dataset)} samples')
        
        # Use EXACTLY the same training arguments as train_moe2.py
        output_dir = moe_save_dir
        
        date_str = datetime.datetime.now().strftime("%d%m%y")
        run_name = f"SimpleMoE_{dataset_name}_{date_str}"
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            run_name=run_name,
            num_train_epochs=epoch if epoch else 10,
            per_device_train_batch_size=16,  # Same as train_moe2.py
            per_device_eval_batch_size=16,   # Same as train_moe2.py
            learning_rate=1e-3,  # Same as train_moe2.py
            weight_decay=0.0,    # Same as train_moe2.py
            warmup_ratio=0.0,    # Same as train_moe2.py
            lr_scheduler_type='constant',  # Same as train_moe2.py
            eval_strategy="epoch",
            save_strategy="epoch",
            save_total_limit=1,
            logging_strategy="steps",
            logging_steps=50,
            report_to=[],
            load_best_model_at_end=True,
            metric_for_best_model='eval_map_50',
            greater_is_better=True,
            fp16=False,  # Same as train_moe2.py
            bf16=False,  # Same as train_moe2.py
            dataloader_num_workers=0,
            gradient_accumulation_steps=1,
            remove_unused_columns=False,
            max_grad_norm=1.0,
            save_safetensors=True,
            eval_do_concat_batches=False,
            disable_tqdm=False,
        )
        
        eval_compute_metrics_fn = get_eval_compute_metrics_fn(image_processor)
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            processing_class=image_processor,
            data_collator=collate_fn,
            compute_metrics=eval_compute_metrics_fn,
        )
        
        print(f"\n=== Training SimpleMoE on {dataset_name} ===")
        
        # CRITICAL: Before training, ensure model components are in correct mode
        model.train()  # MoE model in train mode
        model.classifier.eval()  # But keep classifier in eval mode
        
        # Freeze classifier parameters explicitly
        for param in model.classifier.parameters():
            param.requires_grad = False
        
        trainer.train()
        
        # Save trained MoE model explicitly (in addition to Trainer's automatic saving)
        trained_moe_path = os.path.join(moe_save_dir, 'moe_trained.pth')
        torch.save(model.state_dict(), trained_moe_path)
        print(f"Trained MoE model saved to: {trained_moe_path}")
        
        # Reset routing stats and ensure eval mode before final evaluation
        model.eval()
        model.classifier.eval()
        model.reset_routing_stats()
        
        # Clear CUDA cache before final evaluation
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print(f"\n=== Testing SimpleMoE on {dataset_name} test set ===")
        test_results = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix='test')
        
        # Print test results exactly like train_moe2.py with safe handling
        print(f"\n=== Test Results on {dataset_name} ===")
        if isinstance(test_results, dict):
            for key, value in test_results.items():
                try:
                    if isinstance(value, (int, float, np.integer, np.floating)):
                        print(f"{key}: {float(value):.4f}")
                    else:
                        print(f"{key}: {value}")
                except Exception as e:
                    print(f"{key}: {value} (formatting error: {e})")
        
        # Print final routing statistics (same as train_moe2.py)
        print(f"\n=== Final Routing Statistics on {dataset_name} ===")
        final_stats = model.get_routing_stats()
        for expert_name, usage in final_stats.items():
            print(f"{expert_name}: {usage*100:.1f}%")
        
        # Save final MoE model after testing
        final_moe_path = os.path.join(moe_save_dir, 'moe_final.pth')
        torch.save(model.state_dict(), final_moe_path)
        
        # Save final results in moe_MOMO directory
        # Save final classifier
        final_classifier_path = os.path.join(moe_save_dir, 'classifier_final.pth')
        torch.save(classifier.state_dict(), final_classifier_path)
        
        # Save final results with safe JSON serialization
        final_results_path = os.path.join(moe_save_dir, 'results_final.json')
        with open(final_results_path, 'w') as f:
            json_results = {}
            for k, v in test_results.items():
                try:
                    if isinstance(v, (np.integer, np.floating, np.ndarray)):
                        json_results[k] = float(v)
                    elif isinstance(v, (int, float)):
                        json_results[k] = v
                    else:
                        json_results[k] = str(v)
                except Exception as e:
                    json_results[k] = str(v)
            json_results['routing_stats'] = final_stats
            json.dump(json_results, f, indent=2)
        
        print(f"Final MoE model saved to: {final_moe_path}")
        print(f"Final classifier saved to: {final_classifier_path}")
        print(f"Results saved to: {final_results_path}")
        print(f"All models and results saved to: {moe_save_dir}")
        
        return test_results
        
    except Exception as e:
        print(f"Error in run_moe_like_train_moe2: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_individual_expert_comparison(config, device, dataset_name, expert_weights_dir):
    """So sÃ¡nh káº¿t quáº£ SimpleMoE vs expert riÃªng láº» Ä‘á»ƒ verify."""
    print(f"\n=== COMPARISON: SimpleMoE vs Individual Expert on {dataset_name} ===")
    
    # Load expert models
    expert_models, expert_processors = load_expert_models(expert_weights_dir, device)
    image_processor = expert_processors[0]
    
    # Load classifier
    moe_save_dir = os.path.join(expert_weights_dir, 'moe_MOMO')
    classifier_path = os.path.join(moe_save_dir, 'classifier_best.pth')
    classifier = SimpleDatasetClassifier(num_classes=3, device=device).to(device)
    classifier.load_state_dict(torch.load(classifier_path, map_location=device))
    classifier.eval()
    
    # Create test dataset
    SPLITS_DIR = Path(config.get('dataset', {}).get('splits_dir', '/content/dataset'))
    MODEL_NAME = config.get('model', {}).get('model_name', 'hustvl/yolos-base')
    
    test_dataset = BreastCancerDataset(
        split='test',
        splits_dir=SPLITS_DIR,
        dataset_name=dataset_name,
        image_processor=image_processor,
        model_type=get_model_type(MODEL_NAME),
    )
    
    print(f'Test dataset: {len(test_dataset)} samples')
    
    # Test individual expert
    dataset_map = {'CSAW': 0, 'DMID': 1, 'DDSM': 2}
    expert_idx = dataset_map[dataset_name]
    individual_expert = expert_models[expert_idx]
    
    print(f"\n1. Testing individual {dataset_name} expert...")
    
    training_args = TrainingArguments(
        output_dir='./temp_individual',
        per_device_eval_batch_size=8,
        dataloader_num_workers=0,
        remove_unused_columns=False,
        report_to=[],
        fp16=False,
        eval_do_concat_batches=False,
        disable_tqdm=False,
    )
    
    eval_compute_metrics_fn = get_eval_compute_metrics_fn(image_processor)
    
    individual_trainer = Trainer(
        model=individual_expert,
        args=training_args,
        processing_class=image_processor,
        data_collator=collate_fn,
        compute_metrics=eval_compute_metrics_fn,
    )
    
    individual_results = individual_trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix='individual')
    
    print(f"Individual {dataset_name} Expert Results:")
    for key, value in individual_results.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
    
    # Test SimpleMoE
    print(f"\n2. Testing SimpleMoE on {dataset_name}...")
    
    moe_model = SimpleMoE(expert_models, classifier, device).to(device)
    moe_model.eval()
    
    training_args_moe = TrainingArguments(
        output_dir='./temp_moe',
        per_device_eval_batch_size=8,
        dataloader_num_workers=0,
        remove_unused_columns=False,
        report_to=[],
        fp16=False,
        eval_do_concat_batches=False,
        disable_tqdm=False,
    )
    
    moe_trainer = Trainer(
        model=moe_model,
        args=training_args_moe,
        processing_class=image_processor,
        data_collator=collate_fn,
        compute_metrics=eval_compute_metrics_fn,
    )
    
    moe_results = moe_trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix='moe')
    
    print(f"SimpleMoE Results:")
    for key, value in moe_results.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
    
    # Compare results
    print(f"\n3. Comparison:")
    individual_map50 = individual_results.get('individual_map_50', 0.0)
    moe_map50 = moe_results.get('moe_map_50', 0.0)
    
    print(f"Individual Expert mAP@50: {individual_map50:.4f}")
    print(f"SimpleMoE mAP@50: {moe_map50:.4f}")
    print(f"Difference: {abs(individual_map50 - moe_map50):.4f}")
    
    # Routing stats
    final_stats = moe_model.get_routing_stats()
    print(f"Routing to {dataset_name}: {final_stats.get(dataset_name, 0.0)*100:.1f}%")
    
    if final_stats.get(dataset_name, 0.0) > 0.95:  # 95%+ routing to correct expert
        print("âœ“ Routing is correct (>95%)")
        if abs(individual_map50 - moe_map50) < 0.01:  # Very similar results
            print("âœ“ Results are equivalent (<1% difference)")
        else:
            print("âœ— Results differ significantly")
    else:
        print("âœ— Routing is incorrect (<95%)")
    
    # ADDED: Full mAP evaluation like test.py
    print(f"\n5. Complete mAP Evaluation (like test.py):")
    try:
        # Individual Expert mAP evaluation
        print(f"\n--- Individual {dataset_name} Expert mAP ---")
        individual_trainer = create_trainer_for_evaluation(individual_expert, image_processor, test_dataset)
        individual_results = individual_trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix='individual')
        
        print(f"Individual {dataset_name} Expert Results:")
        for key, value in individual_results.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
        
        # SimpleMoE mAP evaluation
        print(f"\n--- SimpleMoE mAP ---")
        moe_trainer = create_trainer_for_evaluation(moe_model, image_processor, test_dataset)
        moe_results = moe_trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix='moe')
        
        print(f"SimpleMoE Results:")
        for key, value in moe_results.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
        
        # Final comparison with mAP
        individual_map50 = individual_results.get('individual_map_50', 0.0)
        moe_map50 = moe_results.get('moe_map_50', 0.0)
        
        print(f"\n--- Final mAP Comparison ---")
        print(f"Individual Expert mAP@50: {individual_map50:.4f}")
        print(f"SimpleMoE mAP@50: {moe_map50:.4f}")
        print(f"mAP Difference: {abs(individual_map50 - moe_map50):.4f}")
        
        if abs(individual_map50 - moe_map50) < 0.01:
            print("âœ“ mAP results are equivalent!")
        else:
            print("âš ï¸ mAP results differ")
        
        return {
            'individual_loss': avg_loss_individual,
            'moe_loss': avg_loss_moe,
            'individual_map50': individual_map50,
            'moe_map50': moe_map50,
            'routing_stats': final_stats,
            'routing_accuracy': routing_to_target,
            'individual_results': individual_results,
            'moe_results': moe_results
        }
        
    except Exception as e:
        print(f"mAP evaluation failed (evaluation.py bug): {e}")
        print("But loss comparison above is sufficient to verify SimpleMoE correctness")
        
        return {
            'individual_loss': avg_loss_individual,
            'moe_loss': avg_loss_moe,
            'routing_stats': final_stats,
            'routing_accuracy': routing_to_target,
            'evaluation_error': str(e)
        }

def create_trainer_for_evaluation(model, image_processor, test_dataset):
    """Create a trainer for evaluation like test.py."""
    training_args = TrainingArguments(
        output_dir='./temp_eval',
        per_device_eval_batch_size=8,
        dataloader_num_workers=0,
        remove_unused_columns=False,
        report_to=[],
        fp16=False,
        bf16=False,
        eval_do_concat_batches=False,
        disable_tqdm=False,
    )
    
    eval_compute_metrics_fn = get_eval_compute_metrics_fn(image_processor)
    
    trainer = Trainer(
        model=model,
        args=training_args,
        processing_class=image_processor,
        data_collator=collate_fn,
        compute_metrics=eval_compute_metrics_fn,
    )
    
    return trainer

def simple_test_comparison(config, device, dataset_name, expert_weights_dir):
    """Complete test comparison with mAP evaluation like test.py."""
    print(f"\n=== SimpleMoE vs Individual Expert on {dataset_name} (Full Evaluation) ===")
    
    # Load expert models (suppress loading messages)
    expert_models, expert_processors = load_expert_models(expert_weights_dir, device)
    image_processor = expert_processors[0]
    
    # Load classifier (suppress loading messages)
    moe_save_dir = os.path.join(expert_weights_dir, 'moe_MOMO')
    classifier_path = os.path.join(moe_save_dir, 'classifier_best.pth')
    classifier = SimpleDatasetClassifier(num_classes=3, device=device).to(device)
    classifier.load_state_dict(torch.load(classifier_path, map_location=device))
    classifier.eval()
    
    # Create test dataset
    SPLITS_DIR = Path(config.get('dataset', {}).get('splits_dir', '/content/dataset'))
    MODEL_NAME = config.get('model', {}).get('model_name', 'hustvl/yolos-base')
    
    test_dataset = BreastCancerDataset(
        split='test',
        splits_dir=SPLITS_DIR,
        dataset_name=dataset_name,
        image_processor=image_processor,
        model_type=get_model_type(MODEL_NAME),
    )
    
    print(f'Test dataset: {len(test_dataset)} samples')
    
    # Test individual expert first
    dataset_map = {'CSAW': 0, 'DMID': 1, 'DDSM': 2}
    expert_idx = dataset_map[dataset_name]
    individual_expert = expert_models[expert_idx]
    
    # Helper function to move labels to device
    def move_labels_to_device(labels_list, target_device):
        """Move all label tensors to target device."""
        if labels_list is None:
            return None
        
        moved_labels = []
        for label_dict in labels_list:
            moved_dict = {}
            for key, value in label_dict.items():
                if isinstance(value, torch.Tensor):
                    moved_dict[key] = value.to(target_device)
                else:
                    moved_dict[key] = value
            moved_labels.append(moved_dict)
        return moved_labels
    
    # 1. Quick loss comparison (no progress bars)
    print(f"\n1. Loss Comparison:")
    
    # Test individual expert
    individual_expert.eval()
    total_loss = 0.0
    num_batches = 0
    
    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)
    
    with torch.no_grad():
        for batch in test_loader:
            pixel_values = batch['pixel_values'].to(device)
            labels = batch['labels']
            labels = move_labels_to_device(labels, device)
            
            output = individual_expert(pixel_values, labels=labels)
            
            if hasattr(output, 'loss') and output.loss is not None:
                total_loss += output.loss.item()
                num_batches += 1
    
    avg_loss_individual = total_loss / num_batches if num_batches > 0 else 0.0
    
    # Test SimpleMoE
    moe_model = SimpleMoE(expert_models, classifier, device).to(device)
    moe_model.eval()
    moe_model.reset_routing_stats()
    
    total_loss_moe = 0.0
    num_batches_moe = 0
    
    with torch.no_grad():
        for batch in test_loader:
            pixel_values = batch['pixel_values'].to(device)
            labels = batch['labels']
            labels = move_labels_to_device(labels, device)
            
            output = moe_model(pixel_values, labels=labels)
            
            if hasattr(output, 'loss') and output.loss is not None:
                total_loss_moe += output.loss.item()
                num_batches_moe += 1
    
    avg_loss_moe = total_loss_moe / num_batches_moe if num_batches_moe > 0 else 0.0
    
    print(f"Individual {dataset_name} Expert Loss: {avg_loss_individual:.4f}")
    print(f"SimpleMoE Loss: {avg_loss_moe:.4f}")
    print(f"Loss Difference: {abs(avg_loss_individual - avg_loss_moe):.4f}")
    
    # 2. Full mAP evaluation like test.py
    print(f"\n2. Complete mAP Evaluation (like test.py):")
    
    individual_map50 = 0.0
    moe_map50 = 0.0
    individual_results = {}
    moe_results = {}
    
    try:
        # Individual Expert mAP evaluation (exactly like test.py)
        print(f"\n--- Individual {dataset_name} Expert ---")
        
        training_args_individual = TrainingArguments(
            output_dir='./temp_individual',
            per_device_eval_batch_size=8,
            dataloader_num_workers=0,
            remove_unused_columns=False,
            report_to=[],
            fp16=torch.cuda.is_available(),
            eval_do_concat_batches=False,
            disable_tqdm=False,
        )
        
        eval_compute_metrics_fn = get_eval_compute_metrics_fn(image_processor)
        
        individual_trainer = Trainer(
            model=individual_expert,
            args=training_args_individual,
            processing_class=image_processor,
            data_collator=collate_fn,
            compute_metrics=eval_compute_metrics_fn,
        )
        
        individual_results = individual_trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix='individual')
        individual_map50 = individual_results.get('individual_map_50', 0.0)
        
        print(f"Individual {dataset_name} Expert Results:")
        for key, value in individual_results.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
        
        # SimpleMoE mAP evaluation (exactly like test.py)
        print(f"\n--- SimpleMoE ---")
        
        training_args_moe = TrainingArguments(
            output_dir='./temp_moe',
            per_device_eval_batch_size=8,
            dataloader_num_workers=0,
            remove_unused_columns=False,
            report_to=[],
            fp16=torch.cuda.is_available(),
            eval_do_concat_batches=False,
            disable_tqdm=False,
        )
        
        moe_trainer = Trainer(
            model=moe_model,
            args=training_args_moe,
            processing_class=image_processor,
            data_collator=collate_fn,
            compute_metrics=eval_compute_metrics_fn,
        )
        
        moe_results = moe_trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix='moe')
        moe_map50 = moe_results.get('moe_map_50', 0.0)
        
        print(f"SimpleMoE Results:")
        for key, value in moe_results.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
                
    except Exception as e:
        print(f"mAP evaluation failed: {e}")
        print("Using loss comparison only")
    
    # 3. Routing analysis
    final_stats = moe_model.get_routing_stats()
    routing_to_target = final_stats.get(dataset_name, 0.0)
    
    print(f"\n3. Routing Analysis:")
    print(f"Routing Statistics:")
    for expert_name, usage in final_stats.items():
        print(f"  {expert_name}: {usage*100:.1f}%")
    print(f"Routing to correct expert ({dataset_name}): {routing_to_target*100:.1f}%")
    
    # 4. Final comparison (like test.py output format)
    print(f"\n=== Final Comparison (test.py style) ===")
    print(f"Individual {dataset_name} Expert:")
    print(f"  Loss: {avg_loss_individual:.4f}")
    print(f"  mAP@50: {individual_map50:.4f}")
    
    print(f"\nSimpleMoE:")
    print(f"  Loss: {avg_loss_moe:.4f}")
    print(f"  mAP@50: {moe_map50:.4f}")
    
    print(f"\nDifferences:")
    print(f"  Loss Difference: {abs(avg_loss_individual - avg_loss_moe):.4f}")
    print(f"  mAP@50 Difference: {abs(individual_map50 - moe_map50):.4f}")
    
    # 5. Status evaluation
    print(f"\n=== Status ===")
    if routing_to_target > 0.95:
        print("âœ“ Routing is correct (>95%)")
        if abs(avg_loss_individual - avg_loss_moe) < 0.1:
            print("âœ“ Loss results are equivalent")
            if abs(individual_map50 - moe_map50) < 0.01:
                print("âœ“ mAP results are equivalent")
                print("ðŸŽ‰ SimpleMoE is working perfectly!")
            else:
                print("âš ï¸ mAP results differ slightly")
        else:
            print("âš ï¸ Loss results differ - may need investigation")
    else:
        print("âœ— Routing is incorrect (<95%)")
        print("âŒ Classifier may need retraining")
    
    return {
        'individual_loss': avg_loss_individual,
        'moe_loss': avg_loss_moe,
        'individual_map50': individual_map50,
        'moe_map50': moe_map50,
        'routing_stats': final_stats,
        'routing_accuracy': routing_to_target,
        'individual_results': individual_results,
        'moe_results': moe_results
    }

def test_only_mode(config, device, dataset_name, expert_weights_dir):
    """Test-only mode with complete evaluation like test.py."""
    print("\n" + "="*60)
    print("TEST-ONLY MODE: SimpleMoE Complete Evaluation")
    print("="*60)
    
    # Load classifier from moe_MOMO path
    moe_save_dir = os.path.join(expert_weights_dir, 'moe_MOMO')
    classifier_path = os.path.join(moe_save_dir, 'classifier_best.pth')
    
    if not os.path.exists(classifier_path):
        # Try to find classifier_final.pth as backup
        classifier_final_path = os.path.join(moe_save_dir, 'classifier_final.pth')
        if os.path.exists(classifier_final_path):
            classifier_path = classifier_final_path
            print(f"Using final classifier: {classifier_path}")
        else:
            raise FileNotFoundError(f"No classifier found in {moe_save_dir}. Run training first.")
    
    classifier = SimpleDatasetClassifier(num_classes=3, device=device).to(device)
    classifier.load_state_dict(torch.load(classifier_path, map_location=device))
    classifier.eval()
    print(f"Loaded classifier from: {classifier_path}")
    
    # Load expert models
    expert_models, expert_processors = load_expert_models(expert_weights_dir, device)
    image_processor = expert_processors[0]
    
    # Create MoE model
    model = SimpleMoE(expert_models, classifier, device).to(device)
    model.eval()
    print("Created SimpleMoE with loaded classifier")
    
    # Save the created MoE model for test-only mode
    test_moe_path = os.path.join(moe_save_dir, 'moe_test_only.pth')
    torch.save(model.state_dict(), test_moe_path)
    print(f"Test-only MoE model saved to: {test_moe_path}")
    
    print(f'Test dataset ({dataset_name}): processing...')
    
    # Run complete comparison (includes mAP evaluation like test.py)
    try:
        test_results = simple_test_comparison(config, device, dataset_name, expert_weights_dir)
        
        # Save test results
        test_results_path = os.path.join(moe_save_dir, f'complete_test_results_{dataset_name}.json')
        with open(test_results_path, 'w') as f:
            json_results = {}
            for k, v in test_results.items():
                if isinstance(v, (np.integer, np.floating, np.ndarray)):
                    json_results[k] = float(v)
                elif isinstance(v, dict):
                    json_results[k] = v
                else:
                    json_results[k] = str(v)
            json_results['dataset'] = dataset_name
            json.dump(json_results, f, indent=2)
        
        print(f"\nComplete test results saved to: {test_results_path}")
        print(f"Test-only MoE model saved to: {test_moe_path}")
        
        return test_results
        
    except Exception as e:
        print(f"Complete test failed: {e}")
        import traceback
        traceback.print_exc()
        return {'error': str(e)}